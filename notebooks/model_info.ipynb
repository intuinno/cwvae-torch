{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5ca4aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from pathlib import Path\n",
    "import sys  \n",
    "\n",
    "# Get my_package directory path from Notebook\n",
    "parent_dir = str(Path().resolve().parents[0])\n",
    "\n",
    "# Add to sys.path\n",
    "sys.path.insert(0, parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a7f742d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import distributions as torchd\n",
    "\n",
    "import tools\n",
    "import einops\n",
    "from typing import Union\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7041c60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/intuinno/codegit/cwvae-torch'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b698e12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks import Conv3dVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "568920db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Conv3dVAE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa10284e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca9b96c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Conv3dVAE                                [10, 1000, 67, 67, 1]     --\n",
       "├─Sequential: 1-1                        [2500, 4, 2, 16, 16]      --\n",
       "│    └─Conv3d: 2-1                       [2500, 2, 3, 32, 32]      66\n",
       "│    └─ELU: 2-2                          [2500, 2, 3, 32, 32]      --\n",
       "│    └─Conv3d: 2-3                       [2500, 4, 2, 16, 16]      260\n",
       "├─Sequential: 1-2                        [2500, 1, 4, 67, 67]      --\n",
       "│    └─ConvTranspose3d: 2-4              [2500, 2, 3, 33, 33]      258\n",
       "│    └─ConvTranspose3d: 2-5              [2500, 1, 4, 67, 67]      65\n",
       "==========================================================================================\n",
       "Total params: 649\n",
       "Trainable params: 649\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 5.86\n",
       "==========================================================================================\n",
       "Input size (MB): 163.84\n",
       "Forward/backward pass size (MB): 653.64\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 817.48\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "12f5d71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv3dVAE(nn.Module):\n",
    "  \n",
    "  def __init__(self, channels_factor=2, \n",
    "               num_conv_layers=2, \n",
    "               act=nn.ELU,\n",
    "               kernels=(2,4,4),\n",
    "               stride=(1,2,2),\n",
    "               input_num_seq=4,\n",
    "               input_width=64,\n",
    "               input_height=64,\n",
    "               input_channels=1,\n",
    "               temp_abs_factor=4):\n",
    "    super(Conv3dVAE, self).__init__()\n",
    "    self._act = act \n",
    "    \n",
    "    enc_layers =[]\n",
    "    in_channels = input_channels\n",
    "    for level in range(num_conv_layers):\n",
    "      out_channels = in_channels * channels_factor\n",
    "      enc_layers.append(nn.Conv3d(in_channels, \n",
    "                                  out_channels,\n",
    "                                  kernels, stride, \n",
    "                                  padding=(0,1,1)\n",
    "                                  ))\n",
    "      if level < num_conv_layers-1:\n",
    "        enc_layers.append(act())\n",
    "      in_channels = out_channels \n",
    "    self.encoder = nn.Sequential(*enc_layers)\n",
    "    \n",
    "    \n",
    "    dec_layers =[]\n",
    "    in_channels = out_channels \n",
    "    for _ in range(num_conv_layers):\n",
    "      out_channels = in_channels // channels_factor \n",
    "      dec_layers.append(nn.ConvTranspose3d(in_channels, \n",
    "                                           out_channels,\n",
    "                                           kernels,\n",
    "                                           stride,\n",
    "                                           padding=(0,1,1),\n",
    "#                                            output_padding=(0,1,1),\n",
    "                                           ))\n",
    "      if level < num_conv_layers-1:\n",
    "        dec_layers.append(act())\n",
    "      in_channels = out_channels\n",
    "    self.decoder = nn.Sequential(*dec_layers)\n",
    "    self._temp_abs_factor = temp_abs_factor\n",
    "    \n",
    "  def forward(self, x):\n",
    "    # Assume x is (b t h w c)\n",
    "    B, T, H, W, C = x.shape\n",
    "    t1 = T // self._temp_abs_factor\n",
    "    x = einops.rearrange(x, 'b (t t2) h w c -> (b t) c t2 h w', t2=self._temp_abs_factor) \n",
    "    z = self.encoder(x)\n",
    "    z = torch.clip(z, -0.5, 0.5)\n",
    "    # logits = einops.rearrange(logits, 'b c t h w -> b t h w c')\n",
    "    # dist = torchd.OneHotCategoricalStraightThrough(logits=logits)\n",
    "    # dist = torchd.independent.Independent(dist, 3)\n",
    "    # z = dist.rsample()\n",
    "    # dec_z = einops.rearrange(z, 'b t h w c -> b c t h w')\n",
    "    recon = self.decoder(z)\n",
    "    recon = torch.clip(recon, -0.5, 0.5)\n",
    "    recon = einops.rearrange(recon, '(b t1) c t2 h w -> b (t1 t2) h w c', t1=t1)\n",
    "    z = einops.rearrange(z, '(b t1) c t h w -> b t1 h w (c t)', t1=t1)\n",
    "    return recon, z\n",
    "  \n",
    "  def decode(self, emb):\n",
    "    # Assume emb is (b t h w c)\n",
    "    B, T, H, W, C = emb.shape\n",
    "    t2 = C // self._temp_abs_factor\n",
    "    z = einops.rearrange(emb, 'b t h w (c t2) -> (b t) c t2 h w', t2 = t2)\n",
    "    recon = self.decoder(z)\n",
    "    recon = einops.rearrange(recon, '(b t1) c t h w -> b (t1 t) h w c', t1=T)\n",
    "    return recon \n",
    "      \n",
    "    \n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fda69909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Conv3dVAE                                [10, 1000, 64, 64, 1]     --\n",
       "├─Sequential: 1-1                        [2500, 4, 2, 16, 16]      --\n",
       "│    └─Conv3d: 2-1                       [2500, 2, 3, 32, 32]      66\n",
       "│    └─ELU: 2-2                          [2500, 2, 3, 32, 32]      --\n",
       "│    └─Conv3d: 2-3                       [2500, 4, 2, 16, 16]      260\n",
       "├─Sequential: 1-2                        [2500, 1, 4, 64, 64]      --\n",
       "│    └─ConvTranspose3d: 2-4              [2500, 2, 3, 32, 32]      258\n",
       "│    └─ConvTranspose3d: 2-5              [2500, 1, 4, 64, 64]      65\n",
       "==========================================================================================\n",
       "Total params: 649\n",
       "Trainable params: 649\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 5.48\n",
       "==========================================================================================\n",
       "Input size (MB): 163.84\n",
       "Forward/backward pass size (MB): 614.40\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 778.24\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Conv3dVAE()\n",
    "summary(model, input_size=(10,1000,64,64,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc64b0c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a32bf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
